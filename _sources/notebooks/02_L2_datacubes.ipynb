{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"https://notebooks.dtcglaciers.org/_images/ESA_logo.svg\" width=\"160\" align='right'/>\n",
    "</div>\n",
    "\n",
    "# Creating DTC-Glaciers EO-DT-Enhanced Data cubes (L2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "For this example, we focus on the **Brúarjökull outlet glacier** of the Vatnajökull ice cap in Iceland.  \n",
    "Brúarjökull drains into catchments that feed **Iceland’s largest hydropower system**, making it a relevant case study for exploring how variations in **glacier mass balance** influence **seasonal and long-term water availability** (see [this interactive visualisation story on the DestinE website](https://dea.destine.eu/web//stories/viewer/684039a1b62b202b54ee0271)).\n",
    "\n",
    "In the DTC Glaciers concept, **Level 2 (L2) data cubes** build upon the observational foundation provided by **L1 data cubes** by integrating EO observations with physically based glacier modelling and data assimilation.  \n",
    "While L1 data cubes contain harmonised observations only, L2 data cubes provide model-consistent estimates of glacier state and fluxes, together with associated uncertainty information.\n",
    "\n",
    "In this notebook, we use **Brúarjökull** to demonstrate how **L1 observational data cubes** (created in [this notebook](01_L1_datacubes)) are ingested into the DTC Glaciers data assimilation pipeline to generate **L2 data cubes**.  \n",
    "This process illustrates how new EO observations can be systematically combined with modelling frameworks to produce temporally continuous, physically consistent representations of glacier evolution.\n",
    "\n",
    "The example highlights the role of L2 data cubes in bridging observations and projections, providing a basis for improved assessments of future glacier change and its implications for water availability in glacier-influenced hydropower systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "If required, install the DTCG API using the following command:\n",
    "\n",
    "```\n",
    "!pip install 'dtcg[jupyter] @ git+https://github.com/DTC-Glaciers/dtcg'\n",
    "```\n",
    "\n",
    "Run this command in a notebook cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import dtcg.integration.oggm_bindings as oggm_bindings\n",
    "import dtcg.integration.calibration as calibration\n",
    "from dtcg import DEFAULT_L1_DATACUBE_URL, DEFAULT_L2_DATACUBE_URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgi_id_ice = \"RGI60-06.00377\"  # Brúarjökull\n",
    "dtcg_oggm_ice = oggm_bindings.BindingsOggmModel(rgi_id=rgi_id_ice)\n",
    "\n",
    "def get_l1_data_tree(rgi_id):\n",
    "    return xr.open_datatree(f\"{DEFAULT_L1_DATACUBE_URL}{rgi_id}.zarr\",\n",
    "                            engine=\"zarr\",\n",
    "        )\n",
    "\n",
    "data_tree_ice = get_l1_data_tree(rgi_id_ice)\n",
    "data_tree_ice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Assimilating EO-data within DTC-Glaciers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### Preparing L1 data for assimilation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "Here we show how to use observations from **[Hugonnet et al. (2021)](https://doi.org/10.1038/s41586-021-03436-z)** and **[CryoTEMPO-EOLIS](https://cryotempo-eolis.org/)** to create two different versions of **L2 data cubes**.  \n",
    "\n",
    "In [OGGM](https://oggm.org/) (the glacier model used under the hood), the default approach is to use **glacier-integrated elevation change** from **Hugonnet et al. (2021)**, averaged over a **20-year period**, as the geodetic reference.\n",
    "\n",
    "To create a comparable reference from CryoSat-2, we convert the CryoSat-2 elevation-change time series ($\\Delta h$) into a mass-change estimate over a selected period ($\\mathrm{dmda}$). We then compare this with the Hugonnet et al. (2021) reference dataset retrieved from the **[OGGM shop](https://docs.oggm.org/en/stable/shop.html)**.\n",
    "\n",
    "### Steps\n",
    "\n",
    "1. **Compute the elevation change over the chosen period**  \n",
    "\n",
    "   $$\n",
    "      \\Delta h = h(t_{\\mathrm{end}}) - h(t_{\\mathrm{start}})\n",
    "   $$\n",
    "\n",
    "2. **Convert elevation change to mass change per square meter** (to match the Hugonnet units) using a bulk density of  \n",
    "   $\\rho = 850\\ \\mathrm{kg\\,m^{-3}}$:\n",
    "\n",
    "   $$\n",
    "      \\mathrm{dmda}\\;[\\mathrm{kg\\,m^{-2}}]\n",
    "      = \\Delta h\\,\\rho\n",
    "   $$\n",
    "\n",
    "**TODO:** Add a short explanation of how uncertainties are propagated\n",
    "\n",
    "Next, we compare the two reference datasets using the DTC-Glaciers `get_ref_mb` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will use the L1 datacubes for the calculation of the reference mass balance\n",
    "\n",
    "calibrator = calibration.CalibratorCryotempo(l1_datacube=data_tree_ice['L1'].ds,\n",
    "                                             gdir=dtcg_oggm_ice.gdir)\n",
    "\n",
    "ref_mb_cryotempo = calibrator.get_ref_mb(source=\"CryoTEMPO-EOLIS\",\n",
    "                                         ref_mb_period=\"2011-01-01_2019-12-01\",)\n",
    "\n",
    "ref_mb_hugonnet = calibrator.get_ref_mb(source=\"Hugonnet\",\n",
    "                                        ref_mb_period=\"2010-01-01_2020-01-01\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_mb_cryotempo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_mb_hugonnet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "Hugonnet et al. (2021) is way more negative than CryoTEMPO-EOLIS. Be aware that Hugonnet et al. (2021) is given in `kg m-2 yr-1`, while CryoTEMPO-EOLIS is given in `kg m-2`.\n",
    "\n",
    "The reference values from **Hugonnet et al. (2021)** are much more negative than those from **CryoTEMPO-EOLIS**.  \n",
    "When interpreting this difference, keep in mind that the two datasets use **different units**:\n",
    "\n",
    "- **Hugonnet et al. (2021):** `kg m⁻² yr⁻¹` (a *rate* per year)  \n",
    "- **CryoTEMPO-EOLIS:** `kg m⁻²` (a *cumulative change* over the selected period)\n",
    "\n",
    "To compare them directly, you need to convert one dataset so that both represent the **same quantity** (either a total change over a period, or a per-year rate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CryoTEMPO-EOLIS converted to kg m⁻² yr⁻¹\n",
    "ref_mb_cryotempo[0] / 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "### Climate forcing data from DestinE Platform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "For the model climate forcing, we use **ERA5** data downloaded from the **[Earth Data Hub](https://earthdatahub.destine.eu/)**, which is available through the **[DestinE Platform](https://platform.destine.eu/)**. This setup allows us to retrieve **up-to-date climate data** and to generate **L2 data cubes** that can be kept current.\n",
    "\n",
    "In the present implementation, the **precomputed data cubes** are processed up to **October 2025**. However, these can be easily extended using the approach shown below.\n",
    "\n",
    "To run the code in the following section, you need to register on the **DestinE Platform** by following the instructions provided here: [Getting started with Earth Data Hub](https://earthdatahub.destine.eu/getting-started)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment to get up-to-date climate data for one glacier\n",
    "#import xarray as xr\n",
    "\n",
    "#ds = xr.open_dataset(\n",
    "#    \"https://data.earthdatahub.destine.eu/era5/reanalysis-era5-single-levels-monthly-means-v0.zarr\",\n",
    "#    storage_options={\"client_kwargs\":{\"trust_env\":True}},\n",
    "#    chunks={},\n",
    "#    engine=\"zarr\",\n",
    "#)\n",
    "\n",
    "## grid point used by OGGM for Hintereisferner\n",
    "#ds_hef = ds[['t2m', 'tp']].sel(longitude=10.75, latitude=46.75).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or have a look at our downloaded and processed daily data up to October 2025\n",
    "fp_daily = dtcg_oggm_ice.gdir.get_filepath('climate_historical', filesuffix='_era_daily')\n",
    "\n",
    "with xr.open_dataset(fp_daily) as ds_clim_daily:\n",
    "    ds_clim_daily = ds_clim_daily.load()\n",
    "\n",
    "fig, axs = plt.subplots(2, 1)\n",
    "ds_clim_daily.prcp.plot(ax=axs[0])\n",
    "ds_clim_daily.temp.plot(ax=axs[1]);\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "### Data assimilation and uncertainty propagation with Monte Carlo Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "To propagate **model and observation uncertainty** through the **assimilation pipeline**, we use a **Monte Carlo simulation (MCS)**. The goal is to **calibrate a temperature-index mass balance model** so that it matches one of the EO datasets described above.\n",
    "\n",
    "To do this, we define **probability distributions** for all **inputs** used during model calibration. These include both the **EO data** and the **model parameters** (precipitation factor, temperature bias, and degree-day factor). Together, these distributions form a **high-dimensional parameter space** (a calibration hyperspace).\n",
    "\n",
    "From this hyperspace, we draw a **sample of around 100 ensemble members** using **Saltelli’s extension** ([Saltelli, 2002](https://doi.org/10.1016/s0010-4655(02)00280-1)) of the **Sobol′ sequence** ([Sobol′, 2001](https://doi.org/10.1016/s0378-4754(00)00270-6)). The Sobol′ sequence is a widely used **quasi-random, low-discrepancy sequence** that provides an efficient and uniform sampling of the parameter space. Internally, this sampling is implemented using the **[SALib sensitivity analysis library](https://salib.readthedocs.io/en/latest/)**.\n",
    "\n",
    "For **each ensemble member**, we generate a full set of **L2 data cubes**. We then summarise the ensemble output by computing the **quantiles** 0.05, 0.15, 0.25, 0.5 (median), 0.75, 0.85, and 0.95.\n",
    "\n",
    "The final **L2 data cubes** contain these quantiles (stored along the `member` coordinate), as well as a **control run**. The control run represents the model output obtained using only the **most likely values** of all inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "### Running the assimilation and generating L2 data cubes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "For the assimilation, we need to select a **reference dataset**, a **reference period**, and a **mass balance model**. The current default model in **OGGM** is a **monthly temperature-index model**, `MonthlyTIModel` (see the [OGGM v1.6.2 documentation](https://docs.oggm.org/en/v1.6.2/mass-balance-monthly.html)).\n",
    "\n",
    "As part of DTC-Glaciers, we developed a **daily temperature-index model**, `DailyTIModel`, as well as a new mass balance model that includes a **bucket system for snow tracking**, `SfcTypeTIModel`. Both models build on the work of [Schuster et al. (2023)](https://doi.org/10.1017/aog.2023.57) and the [OGGM mass-balance sandbox](https://github.com/OGGM/massbalance-sandbox).\n",
    "\n",
    "If you are interested in using the new **surface-tracking mass balance model**, you can expand the cell below to see an example. Please note that this model is still **under active development**, and the results should therefore be interpreted with caution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "from oggm.core import massbalance\n",
    "from functools import partial\n",
    "\n",
    "MySfcTypeTIModel = partial(\n",
    "    massbalance.SfcTypeTIModel,\n",
    "    mb_model_class=massbalance.DailyTIModel,  # options MonthlyTIModel, DailyTIModel\n",
    "    climate_resolution='daily',  # how often the climate should be applied, options annual, monthly and daily\n",
    "    aging_frequency='monthly',  # how often the buckets getting older, options annual and monthly\n",
    "    ys=2000,  # this defines the year of where we start computing the buckets\n",
    "    spinup_years=6,  # this defines how many buckets we are using and how we create the buckets for ys\n",
    "    store_buckets=False,  # if you want to save buckets on the way, available with .mb_buckets_stored, options annual, monthly, daily\n",
    "    store_buckets_dates=None,  # if you only save buckets at specific dates you can provide a list here in floating year convention\n",
    "    use_previous_mbs=True,  # when setting this to True you can revisit an already calculated mb value\n",
    "    store_snowline=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "For this assimilation example, we define a **calibration matrix** with two calibration runs of the same mass balance model (`DailyTIModel`). Each run uses a different **reference dataset** and **time window**:\n",
    "\n",
    "- **Daily_Hugonnet_2010_2020**: calibrated using the geodetic mass balance from **[Hugonnet et al. (2021)](https://doi.org/10.1038/s41586-021-03436-z)** over the period **2010–2020**.  \n",
    "- **Daily_Cryosat_2011_2020**: calibrated using **[CryoTEMPO-EOLIS](https://cryotempo-eolis.org/)** over the period **2011–2020**.\n",
    "\n",
    "The call to `calibrator.calibrate()` then runs the assimilation for **all entries in the calibration matrix** and generates the corresponding **L2 data cubes**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from oggm.core import massbalance\n",
    "\n",
    "calibrator.set_model_matrix(\n",
    "    name=f\"Daily_Hugonnet_2010_2020\",\n",
    "    model=massbalance.DailyTIModel,\n",
    "    ref_mb_period=\"2010-01-01_2020-01-01\",\n",
    "    source=\"Hugonnet\",\n",
    ")\n",
    "\n",
    "calibrator.set_model_matrix(\n",
    "    name=f\"Daily_Cryosat_2011_2020\",\n",
    "    model=massbalance.DailyTIModel,\n",
    "    ref_mb_period=\"2011-01-01_2019-12-01\",\n",
    "    source=\"CryoTEMPO-EOLIS\",\n",
    "    extra_kwargs={},\n",
    ")\n",
    "\n",
    "# run calibration and generate L2 datacubes\n",
    "l2_datacubes = calibrator.calibrate(\n",
    "    gdir=dtcg_oggm_ice.gdir,\n",
    "    # show some information during the workflow to see what is going on\n",
    "    show_log=True,\n",
    "    # 'nr_samples' defines the ensemble size for the Monte Carlo Simulation,\n",
    "    # for demonstration we set it to 2**1,\n",
    "    mcs_sampling_settings={'nr_samples': 2**1},\n",
    "    # select the output data needed, here we select all available data\n",
    "    datacubes_requested=['monthly', 'annual_hydro', 'daily_smb'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "### Adding L2 data cubes to GeoZarr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "We can use the same `GeoZarrHandler` that we used for the **L1 data cubes** in [this notebook](01_L1_datacubes.ipynb) to add the newly created **L2 data cubes**. At the same time, we can update variable names and metadata to follow the **[CF Convention](https://cfconventions.org/Data/cf-conventions/cf-conventions-1.11/cf-conventions.pdf)**.\n",
    "\n",
    "We then add the **L2 data cubes** one by one to the data tree using the `add_layer` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dtcg.datacube.geozarr import GeoZarrHandler\n",
    "\n",
    "datacube_handler = GeoZarrHandler(data_tree_ice['L1'].ds)\n",
    "\n",
    "for datacube_name in l2_datacubes:\n",
    "    datacube_handler.add_layer(datacubes=l2_datacubes[datacube_name],\n",
    "                               datacube_name=datacube_name)\n",
    "\n",
    "datacube_handler.data_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "## Exploring L2 data cubes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "    <b>IMPORTANT</b>: Please note that the L2 data cubes generated in this notebook use a <b>smaller ensemble size</b> for the Monte Carlo simulation. Therefore, the resulting uncertainties are <b>not comparable</b> to those of the preprocessed L2 data cubes. For this reason, we load the equivalent <b>preprocessed L2 data cubes</b> generated above. Uncomment the next cell only if you really want to inspect the L2 data cubes generated in this notebook.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_l2_data_tree(rgi_id):\n",
    "    return xr.open_datatree(\n",
    "            f\"{DEFAULT_L2_DATACUBE_URL}{rgi_id}.zarr\",\n",
    "            chunks={},\n",
    "            engine=\"zarr\",\n",
    "            consolidated=True,\n",
    "            decode_cf=True,\n",
    "        )\n",
    "datacube_handler = GeoZarrHandler(data_tree=get_l2_data_tree(rgi_id_ice))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "After generating the **L2 data cubes** and adding them to the data tree, we can now explore the results.  \n",
    "In general, each **L2 data cube** contains three datasets:\n",
    "\n",
    "- `'monthly'`\n",
    "- `'annual_hydro'`\n",
    "- `'daily_smb'`\n",
    "\n",
    "They contain different variables at different temporal resolutions.\n",
    "\n",
    "Let’s look at the **monthly glacier volume evolution** for our two realisations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "datacube_handler.data_tree['L2_Daily_Hugonnet_2010_2020']['monthly']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_quantile_bands(ds, x_dim, y_var, y_scaling, ax=None, color=\"C0\",\n",
    "                        alpha=(0.3, 0.25, 0.2), label=\"median\",):\n",
    "\n",
    "    x = ds[x_dim]\n",
    "    # plot quantile bands\n",
    "    bands = [(0.25, 0.75, alpha[0]),\n",
    "             (0.15, 0.85, alpha[1]),\n",
    "             (0.05, 0.95, alpha[2]),]\n",
    "    for q_low, q_high, a in bands:\n",
    "        ax.fill_between(x,\n",
    "                        ds[y_var].sel(member=str(q_low)).isel(rgi_id=0) * y_scaling,\n",
    "                        ds[y_var].sel(member=str(q_high)).isel(rgi_id=0) * y_scaling,\n",
    "                        color=color, alpha=a, linewidth=0, )\n",
    "\n",
    "    # Median line\n",
    "    ax.plot(x, ds[y_var].sel(member='0.5') * y_scaling,\n",
    "            color=color, label=label,)\n",
    "\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = sns.color_palette('colorblind')\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "for L2_name, c in zip(['L2_Daily_Hugonnet_2010_2020', 'L2_Daily_Cryosat_2011_2020'], colors):\n",
    "    plot_quantile_bands(datacube_handler.data_tree[L2_name]['monthly'],\n",
    "                        'time', 'volume', 1e-9,\n",
    "                        ax=ax, label=L2_name, color=c)\n",
    "ax.legend()\n",
    "ax.grid('on')\n",
    "ax.set_ylabel('Volume (km³)')\n",
    "ax.set_xlabel('Year')\n",
    "ax.set_title('Monhtly L2 Datacube example with uncertainties')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "The **shaded areas** show the **quantile bands** (0.25–0.75), (0.15–0.85), and (0.05–0.95), which represent **50%**, **70%**, and **90%** of the ensemble outputs from the **MCS**.\n",
    "\n",
    "We can also examine **glacier runoff** at **annual** or **monthly** resolution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "for L2_name, c in zip(['L2_Daily_Hugonnet_2010_2020', 'L2_Daily_Cryosat_2011_2020'], colors):\n",
    "    plot_quantile_bands(datacube_handler.data_tree[L2_name]['annual_hydro'],\n",
    "                        'time', 'runoff', 1e-9,\n",
    "                        ax=ax, label=L2_name, color=c)\n",
    "\n",
    "ax.legend()\n",
    "ax.grid('on')\n",
    "ax.set_ylabel('Runoff (Mt)')\n",
    "ax.set_xlabel('Year')\n",
    "ax.set_title('Annual L2 Datacube example with uncertainties')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "for ax, L2_name in zip(axs, ['L2_Daily_Hugonnet_2010_2020', 'L2_Daily_Cryosat_2011_2020']):\n",
    "    monthly_runoff = datacube_handler.data_tree[L2_name]['annual_hydro'].runoff_monthly.sel(member='0.5')\n",
    "    monthly_runoff = monthly_runoff.rolling(time=31, center=True, min_periods=1).mean() * 1e-9\n",
    "    monthly_runoff.clip(0).plot(ax=ax, cmap='Blues', cbar_kwargs={'label': 'Runoff (Mt)',}, vmax=1000)\n",
    "    ax.set_title(L2_name)\n",
    "    ax.set_xlabel('Months')\n",
    "    ax.set_ylabel('Years')\n",
    "plt.suptitle('Monthly hydro L2 datacube example, only median shown')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "### Precomputed L2 data cubes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "We prepared **preprocessed L2 data cubes** for our case study regions in **Iceland** and **Austria** (see [this notebook](01_L1_datacubes.ipynb#preprocessed-data-cubes-for-case-study-regions) for an overview of the available glaciers).\n",
    "\n",
    "These preprocessed **L2 data cubes**, together with the corresponding **L1 data cubes**, are stored as **GeoZarr** and can be accessed using the following function by providing the **RGI6 ID** of a glacier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dtcg import DEFAULT_L2_DATACUBE_URL\n",
    "\n",
    "def get_l2_data_tree(rgi_id):\n",
    "    return xr.open_datatree(\n",
    "            f\"{DEFAULT_L2_DATACUBE_URL}{rgi_id}.zarr\",\n",
    "            chunks={},\n",
    "            engine=\"zarr\",\n",
    "            consolidated=True,\n",
    "            decode_cf=True,\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
