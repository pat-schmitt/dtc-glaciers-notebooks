{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "**Important**: This notebook only works together with the oggm dev branch at the moment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# Add Surface Type observations to Datacubes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "Here I demonstrate the workflow for adding surface type observations to the Datacube. This code should be added to https://github.com/DTC-Glaciers/dtcg, following a similar approach to how the eolis data is integrated into the Datacubes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Initialize a gdir as an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from oggm import utils, workflow, cfg, tasks, DEFAULT_BASE_URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.initialize()\n",
    "\n",
    "working_dir = 'working_dir_sfc_type'\n",
    "utils.mkdir(working_dir)\n",
    "cfg.PATHS['working_dir'] = working_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "rgi_ids = ['RGI60-11.00719',  # Vernagtferner\n",
    "           #'RGI60-11.00897',  # Hintereisferner\n",
    "          ]\n",
    "gdirs = workflow.init_glacier_directories(\n",
    "    rgi_ids,  # which glaciers?\n",
    "    prepro_base_url=DEFAULT_BASE_URL,  # where to fetch the data?\n",
    "    from_prepro_level=4,  # what kind of data? \n",
    "    prepro_border=80  # how big of a map?\n",
    ")\n",
    "gdir = gdirs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## add surface type observation to gridded data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "For simplicity, instead of creating a lookup table for the original observation files, I merged them for our region of interest in Austria (only two files per timestamp). If there is still time for further development, we could add the option to use the original files by creating a lookup table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import salem\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is based on oggm.shop.millan22._filter_and_reproj\n",
    "def reproject_single_sfc_type_file(gdir, input_file):\n",
    "    # Subset to avoid mega files\n",
    "    dsb = salem.GeoTiff(input_file)\n",
    "    \n",
    "    x0, x1, y0, y1 = gdir.grid.extent_in_crs(dsb.grid.proj)\n",
    "    with warnings.catch_warnings():\n",
    "        # This can trigger an out of bounds warning\n",
    "        warnings.filterwarnings(\"ignore\", category=RuntimeWarning,\n",
    "                                message='.*out of bounds.*')\n",
    "        dsb.set_subset(corners=((x0, y0), (x1, y1)),\n",
    "                       crs=dsb.grid.proj,\n",
    "                       margin=5)\n",
    "    \n",
    "    data_sfc_types = dsb.get_vardata(var_id=1)\n",
    "    data_uncertainty = dsb.get_vardata(var_id=2)\n",
    "    \n",
    "    # Reproject now\n",
    "    with warnings.catch_warnings():\n",
    "        # This can trigger an out of bounds warning\n",
    "        warnings.filterwarnings(\"ignore\", category=RuntimeWarning,\n",
    "                                message='.*out of bounds.*')\n",
    "        r_data_sfc_types = gdir.grid.map_gridded_data(data_sfc_types, dsb.grid, interp='nearest')\n",
    "        r_data_uncertainty = gdir.grid.map_gridded_data(data_uncertainty, dsb.grid, interp='nearest')\n",
    "\n",
    "    return r_data_sfc_types.data, r_data_uncertainty.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_sfc_type_observation(gdir, data_base_url):\n",
    "    # open gridded data\n",
    "    with xr.open_dataset(gdir.get_filepath('gridded_data')) as ds:\n",
    "        ds = ds.load()\n",
    "\n",
    "    # get all available files of observations from the url, only one per day\n",
    "    response = requests.get(data_base_url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    \n",
    "    files_used = []\n",
    "    sfc_type_dates = []\n",
    "    for link in soup.find_all(\"a\", href=True):\n",
    "        file = link['href']\n",
    "        if file.lower().endswith('.tif'):\n",
    "            date = file.split('_')[0]\n",
    "            if date in sfc_type_dates:\n",
    "                # their is already an observation for this date available\n",
    "                raise ValueError(f'Surface type for {date} already added! '\n",
    "                                 'Check the provided input files, for each '\n",
    "                                 'date only one should be provided!')\n",
    "            sfc_type_dates.append(date)\n",
    "            files_used.append(file)\n",
    "\n",
    "    # prepare structure for data\n",
    "    sfc_type_data = np.zeros((len(sfc_type_dates), *ds['glacier_mask'].shape))\n",
    "    sfc_type_uncertainty = np.zeros((len(sfc_type_dates), *ds['glacier_mask'].shape))\n",
    "\n",
    "    # loop through all files and add one after the other\n",
    "    for i, filename in enumerate(files_used):\n",
    "        # download data\n",
    "        input_file = utils.file_downloader(data_base_url + filename)\n",
    "        \n",
    "        r_data, r_uncertainty = reproject_single_sfc_type_file(gdir, input_file)\n",
    "        sfc_type_data[i, :] = r_data\n",
    "        sfc_type_uncertainty[i, :] = r_uncertainty\n",
    "\n",
    "    # use nan for missing data\n",
    "    missing_data_val = np.nan\n",
    "    sfc_type_data = np.where(sfc_type_data == 255,\n",
    "                             missing_data_val, sfc_type_data)\n",
    "    sfc_type_uncertainty = np.where(sfc_type_uncertainty == 255,\n",
    "                                    missing_data_val, sfc_type_uncertainty)\n",
    "\n",
    "    # add to gridded data with some attributes\n",
    "    ds.coords['t_sfc_type'] = pd.to_datetime(sfc_type_dates, format=\"%Y%m%d\")\n",
    "\n",
    "    ds['t_sfc_type'].attrs = {\n",
    "        'long_name': 'Timestamp of surface type observations'\n",
    "    }\n",
    "    ds['sfc_type_data'] = (('t_sfc_type', 'y', 'x'), sfc_type_data)\n",
    "    ds['sfc_type_data'].attrs = {\n",
    "        'long_name': 'Glacier facies classification',\n",
    "        'data_source': 'ENVEO',\n",
    "        'units': 'none',\n",
    "        'code_description': 'Extract dict with ast.literal_eval(ds.code).',\n",
    "        'code': str({\n",
    "            0: 'unclassified',\n",
    "            1: 'snow',\n",
    "            2: 'firn / old snow / bright ice',\n",
    "            3: 'clean ice',\n",
    "            4: 'debris',\n",
    "            5: 'cloud',\n",
    "            'nan': 'no data',\n",
    "        }),\n",
    "    }\n",
    "\n",
    "    ds['sfc_type_uncertainty'] = (('t_sfc_type', 'y', 'x'), sfc_type_uncertainty)\n",
    "    ds['sfc_type_uncertainty'].attrs = {\n",
    "        'long_name': 'Glacier facies classification uncertainty',\n",
    "        'data_source': 'ENVEO',\n",
    "        'units': 'none',\n",
    "        'code_description': 'Extract dict with ast.literal_eval(ds.code).',\n",
    "        'code': str({\n",
    "            1: 'low uncertainty for illuminated pixel',\n",
    "            2: 'medium uncertainty for illuminated pixel',\n",
    "            3: 'high uncertainty for illuminated pixel',\n",
    "            5: 'cloud',\n",
    "            11: 'low uncertainty for shaded pixel',\n",
    "            12: 'medium uncertainty for shaded pixel',\n",
    "            13: 'high uncertainty for shaded pixel',\n",
    "            'nan': 'no data',\n",
    "        }),\n",
    "    }\n",
    "\n",
    "    ds = ds.sortby('t_sfc_type')\n",
    "\n",
    "    ds.to_netcdf(gdir.get_filepath('gridded_data'))\n",
    "\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "data_base_url = 'https://cluster.klima.uni-bremen.de/~dtcg/test_files/case_study_regions/austria/sfc_type_obs/merged_data/'\n",
    "ds_test = add_sfc_type_observation(gdir, data_base_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "with xr.open_dataset(gdir.get_filepath('gridded_data')) as ds:\n",
    "    ds = ds.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "ds.sfc_type_data.isel(t_sfc_type=0).plot()\n",
    "print(ast.literal_eval(ds.sfc_type_data.code))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.sfc_type_uncertainty.isel(t_sfc_type=0).plot()\n",
    "print(ast.literal_eval(ds.sfc_type_uncertainty.code))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "# Generate a snow line observation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "This part prepares the surface type observations to be used during calibration by generating a snowline altitude. In terms of `dtcg`, this is equivalent to creating a time series from the eolis gridded data, making the gridded observations usable for model calibration.\n",
    "\n",
    "**This is still under development and will likely need further refinement, depending on the time available for additional work.**\n",
    "\n",
    "Current approach:\n",
    "- If more than 50% of the total area is cloud-covered, the date will be ignored.\n",
    "- Calculate the relative snow cover area fraction per elevation bin (ignoring cloud-covered pixels).\n",
    "- Use three thresholds (25%, 50%, and 75%) to extract the snowline altitude along with an associated uncertainty.\n",
    "- Currently, the provided pixel uncertainty is ignored."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## Define elevation bands used for snow line creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "Here, we define elevation bands using the model flowline and also include the maximum and minimum values of the DEM to ensure that no observations are missed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_elev_band_edges(gdir, topo_data='topo_smoothed'):\n",
    "    # load flowline\n",
    "    fl_inv = gdir.read_pickle('inversion_flowlines')\n",
    "    assert len(fl_inv) == 1, 'Only works with one flowline.'\n",
    "    fl_inv = fl_inv[0]\n",
    "\n",
    "    # get max and min height of glacier outline\n",
    "    with xr.open_dataset(gdir.get_filepath('gridded_data')) as ds:\n",
    "        glacier_topo_flat = ds[topo_data].data[ds.glacier_mask.astype(bool)]\n",
    "        max_glacier_elevation = np.max(glacier_topo_flat)\n",
    "        min_glacier_elevation = np.min(glacier_topo_flat)\n",
    "\n",
    "    # define edges just in between two surface heights\n",
    "    sfc_h = fl_inv.surface_h\n",
    "    mid = (sfc_h[:-1] + sfc_h[1:]) / 2\n",
    "    elev_band_edges = np.concatenate(\n",
    "        [[max_glacier_elevation * 1.01],  # upper edge, add a little to contain all values\n",
    "         mid,\n",
    "         [min_glacier_elevation * 0.99],  # lower edge, subtract a little to contain all values\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return elev_band_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "elev_band_edges = get_elev_band_edges(gdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "elev_band_edges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "## Count surface types and uncertainty flags per elevation band"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_categories_per_elevation_band(gdir,\n",
    "                                      topo_data='topo_smoothed',\n",
    "                                      category_data='sfc_type_data',\n",
    "                                      category_uncertainty='sfc_type_uncertainty',\n",
    "                                      category_time_var='t_sfc_type',\n",
    "                                      nodata_value = np.nan,\n",
    "                                     ):\n",
    "    \n",
    "    # open needed data for aggregation\n",
    "    with xr.open_dataset(gdir.get_filepath('gridded_data')) as ds:\n",
    "        elevations = ds[topo_data].data[ds.glacier_mask.astype(bool)]\n",
    "        categories = ds[category_data].data[:,ds.glacier_mask.astype(bool)]\n",
    "        categories_attrs = ds[category_data].attrs\n",
    "        uncertainties = ds[category_uncertainty].data[:,ds.glacier_mask.astype(bool)]\n",
    "        uncertainties_attrs = ds[category_uncertainty].attrs\n",
    "        time_dim = categories.shape[0]\n",
    "        time_values = ds[category_time_var]\n",
    "        # Mask out nodata\n",
    "        if np.isnan(nodata_value):\n",
    "            valid_mask = ~np.isnan(categories)\n",
    "        else:\n",
    "            valid_mask = ~np.isin(categories, nodata_value)\n",
    "    \n",
    "    # get elevation bands, need to be ascending for np.digitize\n",
    "    elev_band_edges = get_elev_band_edges(gdir, topo_data=topo_data)\n",
    "    n_bins = elev_band_edges.size - 1\n",
    "    \n",
    "    # assign elevation band indexes, -1 to start from 0\n",
    "    bin_ids = np.digitize(elevations, bins=elev_band_edges) - 1\n",
    "    \n",
    "    def count_values_per_elevation_bin(data):\n",
    "        # Get unique valid categories (excluding nodata and -1)\n",
    "        unique_values = np.unique(data)\n",
    "        unique_values = np.sort(unique_values)\n",
    "        if np.isnan(nodata_value):\n",
    "            unique_values = unique_values[~np.isnan(unique_values)]\n",
    "        else:\n",
    "            unique_values = np.setdiff1d(unique_values, [nodata_value])\n",
    "        # we assign values from 0 to len(unique_cats) to be able to use hist 2d later\n",
    "        values_to_index = {value: i for i, value in enumerate(unique_values)}\n",
    "    \n",
    "        # Create result array: shape should be (time, elevation_bin, valid categories)\n",
    "        result = np.zeros((time_dim, n_bins, len(unique_values)), dtype=int)\n",
    "        \n",
    "        # Efficient per-time counting\n",
    "        for t in range(time_dim):\n",
    "            valid = valid_mask[t]\n",
    "            binned = bin_ids[valid]\n",
    "            datavals = data[t][valid]\n",
    "        \n",
    "            # Map category values to indices\n",
    "            value_idx = np.array([values_to_index[c] for c in datavals])\n",
    "            hist2d = np.zeros((n_bins, len(unique_values)), dtype=int)\n",
    "        \n",
    "            # Use np.add.at for fast 2D histogram\n",
    "            np.add.at(hist2d, (binned, value_idx), 1)\n",
    "            result[t] = hist2d\n",
    "    \n",
    "        return result, unique_values\n",
    "    \n",
    "    category_counts, unique_cats = count_values_per_elevation_bin(categories)\n",
    "    uncertainty_counts, unique_uncert = count_values_per_elevation_bin(uncertainties)\n",
    "    \n",
    "    # create dataset of result\n",
    "    ds = xr.Dataset(\n",
    "        data_vars={\n",
    "            \"category_counts\": (\n",
    "                (\"t_sfc_type\", \"elevation_bin\", \"category\"),\n",
    "                category_counts,\n",
    "                {\"long_name\": \"Count of glacier facies classification per elevation band\"},\n",
    "            ),\n",
    "            \"uncertainty_counts\": (\n",
    "                (\"t_sfc_type\", \"elevation_bin\", \"uncertainty_flag\"),\n",
    "                uncertainty_counts,\n",
    "                {\"long_name\": \"Count of glacier facies classification  uncertainty per elevation band\"},\n",
    "            ),\n",
    "        },\n",
    "        coords={\n",
    "            \"t_sfc_type\": time_values,\n",
    "            \"elevation_bin\": ((\"elevation_bin\"), range(n_bins),\n",
    "                              {\"long_name\": \"Index of elevation bin\"}),\n",
    "            \"lower_elevation\": ((\"elevation_bin\"), elev_band_edges[1:],\n",
    "                                {\"long_name\": \"Lower boundary of elevation bin\"}),\n",
    "            \"upper_elevation\": ((\"elevation_bin\"), elev_band_edges[:-1],\n",
    "                                {\"long_name\": \"Upper boundary of elevation bin\"}),\n",
    "            \"category\": ((\"category\"), unique_cats, categories_attrs),\n",
    "            \"uncertainty_flag\": ((\"uncertainty_flag\"), unique_uncert, uncertainties_attrs),\n",
    "        },\n",
    "    )\n",
    "    \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_counts = get_categories_per_elevation_band(gdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "## Define snowline altitude for each time stamp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "This part likely requires further development. The main question is how to use the information from the category count together with the uncertainty count to derive a snowline altitude with associated uncertainty. Currently the provided observation undertainty is not used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "### exclude elevation bins with no valid data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "This is happening because we use different outline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exclude_empty_elevation_bins(ds):\n",
    "    # get the total number of observations per elevation band\n",
    "    number_grid_points_elev_bin = ds.category_counts.sum(dim='category')\n",
    "\n",
    "    # check that this number is the same for each timestamp\n",
    "    assert np.all(number_grid_points_elev_bin == number_grid_points_elev_bin.isel(t_sfc_type=0))\n",
    "    \n",
    "    # if the number of grid points is 0 their is no valid data\n",
    "    return ds.isel(elevation_bin=np.where(number_grid_points_elev_bin.isel(t_sfc_type=0) != 0)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test it\n",
    "exclude_empty_elevation_bins(ds_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "Three elevation bins less because of the use of a smaller outline compared to RGI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "### exclude dates with more than 50% cloud cover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exclude_dates_with_to_much_cloud_cover(ds, cloud_cover=0.5):\n",
    "    relative_cloud_cover = (ds.category_counts.sum(dim='elevation_bin').sel(category=5) /\n",
    "                            ds.category_counts.sum(dim=['elevation_bin', 'category']))\n",
    "\n",
    "    return ds.isel(t_sfc_type=np.where(relative_cloud_cover < 0.5)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_dates_with_to_much_cloud_cover(ds_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "Five dates with more than 50% cloud cover."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "### define snowline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "what it should do:\n",
    "- exclude elevation bins which are completly cloude covered\n",
    "- exclude the cloud cover per elevation bin when calculating relative snow fraction (we assume all surface types are equally covored by clouds)\n",
    "- flag if complete snow covered (snowline=-np.inf) or complete snow free (snowline=np.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_snowline(ds, thresholds=[0.25, 0.5, 0.75]):\n",
    "    # the provided ds should already be cleaned for scenes \n",
    "    da_counts = ds.category_counts\n",
    "    \n",
    "    all_grid_points_elev_bin = da_counts.sum(dim='category')\n",
    "    cloud_grid_points = da_counts.sel(category=5)\n",
    "    \n",
    "    # exclude completely cloud covered elevation bins\n",
    "    da_cloudfree = da_counts.isel(elevation_bin=np.where(all_grid_points_elev_bin != cloud_grid_points)[0])\n",
    "    \n",
    "    # calculate relative contributions, excluding cloud grid points\n",
    "    # we assume all surface types are equally covored by clouds\n",
    "    da_rel_snow = da_cloudfree.sel(category=1) / da_cloudfree.sel(category=[1, 2, 3, 4]).sum(dim='category')\n",
    "    \n",
    "    # use different thresholds of the snow fraction for integrating uncertainty\n",
    "    def get_lowest_elev_bin_exceeding_threshold(threshold):\n",
    "        exceeding_threshold = da_rel_snow > threshold\n",
    "\n",
    "        # check if lowest bin is snow covered\n",
    "        if exceeding_threshold.isel(elevation_bin=-1):\n",
    "            return da_counts.elevation_bin.values[-1] + 1, -np.inf\n",
    "        # check if no bin exceeds threshold\n",
    "        elif not np.any(exceeding_threshold):\n",
    "            return -1, np.inf\n",
    "        else:\n",
    "            lowest_bin = da_rel_snow.sel(elevation_bin=exceeding_threshold).isel(elevation_bin=-1)\n",
    "            return lowest_bin.elevation_bin.values.item(), lowest_bin.lower_elevation.values.item()\n",
    "    \n",
    "    lowest_elev_bin_idx = []\n",
    "    lowest_elev_bin_values = []\n",
    "    for threshold in thresholds:\n",
    "        tmp_idx, tmp_value = get_lowest_elev_bin_exceeding_threshold(threshold)\n",
    "        lowest_elev_bin_idx.append(tmp_idx)\n",
    "        lowest_elev_bin_values.append(tmp_value)\n",
    "\n",
    "    return lowest_elev_bin_idx, lowest_elev_bin_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "### look at result of derived snowline from observation per timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "These plots are intended for development purposes. They display the following:\n",
    "- The absolute pixel count per elevation bin (first plot).\n",
    "- The relative count, including clouds (second plot).\n",
    "- The uncertainty count (third plot)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_obs = get_categories_per_elevation_band(gdir)\n",
    "ds_obs = exclude_empty_elevation_bins(ds_obs)\n",
    "ds_obs = exclude_dates_with_to_much_cloud_cover(ds_obs, cloud_cover=0.5)\n",
    "\n",
    "for date in ds_obs.t_sfc_type:\n",
    "    ds_use = ds_obs.sel(t_sfc_type=date)\n",
    "\n",
    "    bin_idx, bin_elev = get_snowline(ds_use)\n",
    "    \n",
    "    fig, axs = plt.subplots(1, 3, figsize=(10, 5), gridspec_kw={'wspace':0.1})\n",
    "    \n",
    "    ax_total = axs[0]\n",
    "    ax_relative = axs[1]\n",
    "    ax_uncertainty = axs[2]\n",
    "    \n",
    "    def plot_horizontal_stacked_bar(ax, ds, categories, var, normalize=False):\n",
    "        category_dict = ast.literal_eval(ds[categories].attrs['code'])\n",
    "        previous_values = None\n",
    "        for category in ds_use[categories]:\n",
    "            current_values = ds.loc[{categories: category}][var].values\n",
    "            if normalize:\n",
    "                number_grid_points_elev_bin = ds.category_counts.sum(dim='category')\n",
    "                current_values = current_values.astype(float) / number_grid_points_elev_bin\n",
    "            ax.barh(ds.elevation_bin, current_values,\n",
    "                     left=previous_values,\n",
    "                     label=category_dict[int(category)])\n",
    "        \n",
    "            if previous_values is None:\n",
    "                previous_values = current_values\n",
    "            else:\n",
    "                previous_values = previous_values + current_values\n",
    "    \n",
    "        ax.invert_yaxis()\n",
    "    \n",
    "    plot_horizontal_stacked_bar(ax_total, ds_use, 'category', 'category_counts')\n",
    "    plot_horizontal_stacked_bar(ax_relative, ds_use, 'category', 'category_counts', normalize=True)\n",
    "    plot_horizontal_stacked_bar(ax_uncertainty, ds_use, 'uncertainty_flag', 'uncertainty_counts', normalize=True)\n",
    "\n",
    "    def add_snowline(ax):\n",
    "        if np.isneginf(bin_elev[1]):\n",
    "            extra_label = 'fully snow covered'\n",
    "        elif np.isposinf(bin_elev[1]):\n",
    "            extra_label = 'fully snow free'\n",
    "        else:\n",
    "            extra_label = f'{bin_elev[1]:.0f} m'\n",
    "        ax.axhline(bin_idx[0], color='k', linestyle='--')\n",
    "        ax.axhline(bin_idx[1], color='k', linestyle='-', label=f'snowline ({extra_label})')\n",
    "        ax.axhline(bin_idx[2], color='k', linestyle='--', label='snowline uncertainty')\n",
    "\n",
    "    def set_legend(ax):\n",
    "        ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.05))\n",
    "    \n",
    "    ax_total.set_yticks(ds_use.elevation_bin, [int(elev) for elev in ds_use.lower_elevation.values])\n",
    "    ax_total.set_ylabel('Altitude [m]')\n",
    "    ax_total.set_title('category count')\n",
    "    add_snowline(ax_total)\n",
    "    set_legend(ax_total)\n",
    "    \n",
    "    ax_relative.set_title('category count relative')\n",
    "    ax_relative.set_yticklabels([])\n",
    "    add_snowline(ax_relative)\n",
    "    set_legend(ax_relative)\n",
    "    \n",
    "    ax_uncertainty.set_title('uncertainty count relative')\n",
    "    ax_uncertainty.set_yticklabels([])\n",
    "    add_snowline(ax_uncertainty)\n",
    "    set_legend(ax_uncertainty)\n",
    "\n",
    "    fig.suptitle(date.values)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "# Compare snowline to SfcTypeTIModel without using snowline for calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from oggm.core.massbalance import DailyTIModel, SfcTypeTIModel\n",
    "from functools import partial\n",
    "from oggm.utils import ModelSettings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "## Get SfcTypeTIModel and calibrate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49",
   "metadata": {},
   "source": [
    "**The currently applied calibration is just a preliminary setting and will require further consideration to determine the best approach!**\n",
    "\n",
    "At the moment, I have decided to use the precipitation factor for calibration and aim to leave the melt factor unchanged. The melt factor (the second parameter) will only be adjusted if it is not possible to match the mass balance using the precipitation factor alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add daily climate\n",
    "workflow.execute_entity_task(tasks.process_gswp3_w5e5_data,\n",
    "                                 gdirs, daily=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use the default values of OGGM prepro for the first guess variables\n",
    "default_mb_calib = gdirs[0].read_json('mb_calib')\n",
    "\n",
    "MySfcTypeTIModel = partial(\n",
    "    SfcTypeTIModel,\n",
    "    mb_model_class=DailyTIModel,  # options MonthlyTIModel, DailyTIModel\n",
    "    climate_resolution='daily',  # how often the climate should be applied, options annual, monthly and daily\n",
    "    aging_frequency='monthly',  # how often the buckets getting older, options annual and monthly\n",
    "    ys=2000,  # this defines the year of where we start computing the buckets\n",
    "    spinup_years=6,  # this defines how many buckets we are using and how we create the buckets for ys\n",
    "    store_buckets=False,  # if you want to save buckets on the way, available with .mb_buckets_stored, options annual, monthly, daily\n",
    "    store_buckets_dates=None,  # if you only save buckets at specific dates you can provide a list here in floating year convention\n",
    "    use_previous_mbs=True,  # when setting this to True you can revisit an already calculated mb value\n",
    ")\n",
    "\n",
    "for gdir in gdirs:\n",
    "    ModelSettings(gdir, filesuffix='_daily_sfc', parent_filesuffix='')\n",
    "\n",
    "# now redo the calibration\n",
    "workflow.execute_entity_task(\n",
    "    tasks.mb_calibration_from_hugonnet_mb,\n",
    "    gdirs, settings_filesuffix='_daily_sfc',\n",
    "    observations_filesuffix='_dialy_sfc',  # new handling of observations\n",
    "    informed_threestep=False,  # this was calibrated with monthly, but still use it\n",
    "    overwrite_gdir=True,\n",
    "    calibrate_param1='prcp_fac',\n",
    "    calibrate_param2='melt_f',\n",
    "    calibrate_param3='temp_bias',\n",
    "    prcp_fac=default_mb_calib['prcp_fac'],\n",
    "    melt_f=default_mb_calib['melt_f'],\n",
    "    mb_model_class=MySfcTypeTIModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52",
   "metadata": {},
   "source": [
    "## Get snow buckets for observation dates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53",
   "metadata": {},
   "source": [
    "Here, I demonstrate how to obtain the model counterparts for the observations. Specifically, I create a float year time series based on the actual dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from oggm.utils import date_to_floatyear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data and exclude some stuff, the resulting dates are the one we are intereseted in\n",
    "ds_obs = get_categories_per_elevation_band(gdir)\n",
    "ds_obs = exclude_empty_elevation_bins(ds_obs)\n",
    "ds_obs = exclude_dates_with_to_much_cloud_cover(ds_obs, cloud_cover=0.5)\n",
    "\n",
    "# convert the dates into floatyears\n",
    "dates = ds_obs.t_sfc_type.values\n",
    "years = dates.astype('datetime64[Y]').astype(int) + 1970\n",
    "months = dates.astype('datetime64[M]').astype(int) % 12 + 1\n",
    "days = (dates.astype('datetime64[D]') - dates.astype('datetime64[M]')).astype(int) + 1\n",
    "\n",
    "float_years = date_to_floatyear(\n",
    "    y=years,\n",
    "    m=months,\n",
    "    d=days)\n",
    "\n",
    "float_years_dict = dict(zip(float_years, dates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the mb_model and tell which buckets we are intereseted in\n",
    "mb_model = MySfcTypeTIModel(\n",
    "    gdirs[0],\n",
    "    settings_filesuffix='_daily_sfc',\n",
    "    store_buckets=True,\n",
    "    store_buckets_dates=float_years\n",
    "                            )\n",
    "\n",
    "# by calling the annual mb for 2019 all previous timesteps need to be calculated and the all buckets are available\n",
    "mb_model.get_annual_mb(mb_model.fl.surface_h, year=2019);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57",
   "metadata": {},
   "source": [
    "## get modelled snowline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58",
   "metadata": {},
   "source": [
    "Here is a function for obtaining the modeled snowline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from oggm.utils import floatyear_to_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_modelled_snowline(gdir, years, settings_filesuffix):\n",
    "    # initialize seleceted mb_model\n",
    "    mb_model = MySfcTypeTIModel(\n",
    "        gdir,\n",
    "        settings_filesuffix=settings_filesuffix,\n",
    "        store_buckets=True,\n",
    "        store_buckets_dates=years\n",
    "                            )\n",
    "    # get modelled data\n",
    "    mb_model.get_annual_mb(mb_model.fl.surface_h, year=2019)\n",
    "\n",
    "    # generate snow lines\n",
    "    snow_line = []\n",
    "    snow_line_years = []\n",
    "    for yr in years:\n",
    "        try:\n",
    "            mb_bucket = mb_model.mb_buckets_stored[yr]\n",
    "        except KeyError:\n",
    "            continue\n",
    "    \n",
    "        # we use all layers from October onward\n",
    "        number_layers = floatyear_to_date(yr)[1] + 3  # 3 for Oct, Nov, Dec\n",
    "        layers_of_interest = [f'firn_{i}' for i in range(1, number_layers + 1)]\n",
    "    \n",
    "        not_melted_layers = np.any(mb_bucket[layers_of_interest] > 0, axis=1)\n",
    "\n",
    "        # check if completely snow free\n",
    "        if not np.any(not_melted_layers):\n",
    "            snow_line.append(np.inf)\n",
    "        # check if fully snow covered\n",
    "        elif np.all(not_melted_layers):\n",
    "            snow_line.append(-np.inf)\n",
    "        # otherwise get the lowest elevation band with snow cover\n",
    "        else:\n",
    "            snow_line.append(mb_model.fl.surface_h[not_melted_layers][-1])\n",
    "        snow_line_years.append(yr)\n",
    "\n",
    "    snow_line_years = np.array([float_years_dict[yr] for yr in snow_line_years])\n",
    "\n",
    "    return snow_line, snow_line_years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "snow_line, snow_line_years = get_modelled_snowline(\n",
    "    gdir, float_years, settings_filesuffix='_daily_sfc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62",
   "metadata": {},
   "source": [
    "## plot for comparision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63",
   "metadata": {},
   "source": [
    "A first plot for visual comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "\n",
    "# get observations\n",
    "ds_obs = get_categories_per_elevation_band(gdir)\n",
    "ds_obs = exclude_empty_elevation_bins(ds_obs)\n",
    "ds_obs = exclude_dates_with_to_much_cloud_cover(ds_obs, cloud_cover=0.5)\n",
    "\n",
    "snowline_obs = []\n",
    "snowline_obs_lower = []\n",
    "snowline_obs_upper = []\n",
    "for date in ds_obs.t_sfc_type:\n",
    "    ds_use = ds_obs.sel(t_sfc_type=date)\n",
    "\n",
    "    tmp_idx, tmp_obs = get_snowline(ds_use)\n",
    "    snowline_obs_lower.append(tmp_obs[0])  # 25% snow cover\n",
    "    snowline_obs.append(tmp_obs[1])  # 50% snow cover\n",
    "    snowline_obs_upper.append(tmp_obs[2])  # 75% snow cover\n",
    "\n",
    "# get model output\n",
    "snowline_mdl, snowline_mdl_years = get_modelled_snowline(\n",
    "    gdir, float_years, settings_filesuffix='_daily_sfc')\n",
    "\n",
    "# deal with special cases completely snow covered or completely snow free\n",
    "all_snowlines = np.concatenate((snowline_mdl, snowline_obs_lower, snowline_obs_upper))\n",
    "min_height = np.min(all_snowlines[~np.isinf(all_snowlines)])\n",
    "max_height = np.max(all_snowlines[~np.isinf(all_snowlines)])\n",
    "# set the plotting limit 100 apart of extreme values\n",
    "neg_inf_value = min_height - 150\n",
    "neg_inf_plotting_area = [min_height - 100, min_height - 200]\n",
    "pos_inf_value = max_height + 150\n",
    "pos_inf_plotting_area = [max_height + 100, max_height + 200]\n",
    "\n",
    "def set_infs(var):\n",
    "    var = np.where(np.isposinf(var), pos_inf_value, var)\n",
    "    return np.where(np.isneginf(var), neg_inf_value, var)\n",
    "\n",
    "# plot observations with errorbars\n",
    "snowline_err = np.array(\n",
    "    tuple(zip(set_infs(snowline_obs) - set_infs(snowline_obs_lower),\n",
    "              set_infs(snowline_obs_upper) - set_infs(snowline_obs)))).T\n",
    "ax.errorbar(ds_obs.t_sfc_type.values, set_infs(snowline_obs), yerr=snowline_err,\n",
    "            fmt='o', ms=6, ecolor='black', c='k', capsize=2, lw=0.5,\n",
    "            label='Observations')\n",
    "\n",
    "# plot model output\n",
    "ax.plot(snowline_mdl_years, set_infs(snowline_mdl),\n",
    "        marker='o', ms=4, ls='-', lw=0.5, zorder=5, color='gray',\n",
    "        label='OGGM snow tracking')\n",
    "\n",
    "# add special areas\n",
    "ax.axhspan(neg_inf_plotting_area[0], neg_inf_plotting_area[1],\n",
    "           color='lightblue', alpha=0.5, label='completly snow covered')\n",
    "ax.axhspan(pos_inf_plotting_area[0], pos_inf_plotting_area[1],\n",
    "           color='red', alpha=0.5, label='completly snow free')\n",
    "\n",
    "ax.legend()\n",
    "ax.grid('on')\n",
    "ax.set_title('Snowline Altitude')\n",
    "ax.set_ylabel('Altitude [m]')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65",
   "metadata": {},
   "source": [
    "# Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66",
   "metadata": {},
   "source": [
    "The next steps would be to use the snowline observations for calibration. Some thoughts on this (open for discussion):\n",
    "\n",
    "- Use the precipitation factor as a control parameter for matching the snowline.\n",
    "- Whenever a new precipitation factor is defined, the melt factor should be calibrated to match Hugonnet before obtaining the modeled snowlines.\n",
    "- When comparing observed and modeled snowlines, we need to handle special cases:\n",
    "  - Fully snow-covered areas (`snowline = -np.inf`).\n",
    "  - No snow cover (`snowline = np.inf`).\n",
    "  - Additionally, if we want to be precise, since the outline used for creating the observations is smaller than the one we are using, if the observation shows full snow cover, we should ensure that the modeled snow cover is at least as large as the observation. However, for a first attempt, this can be ignored.\n",
    "- Before minimizing automatically, check if the defined cost function has a minimum by testing an array of precipitation factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
